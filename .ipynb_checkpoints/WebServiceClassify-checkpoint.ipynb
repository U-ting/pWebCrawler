{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/Document/Literature/SourceCode/apis/api_8459.json', 'rb') as f:\n",
    "    rows = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('api_8459.csv', 'a+', newline = '',encoding = 'gb18030')\n",
    "csv_write = csv.writer(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_write.writerow(rows[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 循环里面的字典，将value作为数据写入进去\n",
    "for row in rows:\n",
    "    csv_write.writerow(row.values())\n",
    "    \n",
    "# 关闭打开的文件\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入接下来分析用的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载并检查数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('E:/Document/Literature/SourceCode/api_8459.xlsx',encoding='ISO-8859-1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8459 entries, 0 to 8458\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   api_id              8459 non-null   int64 \n",
      " 1   api_name            8459 non-null   object\n",
      " 2   api_prim_cate       8459 non-null   int64 \n",
      " 3   api_desc            8459 non-null   object\n",
      " 4   n_followers         8459 non-null   int64 \n",
      " 5   n_appear_in_mashup  8459 non-null   int64 \n",
      " 6   order               8459 non-null   int64 \n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 462.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.head(10)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 nltk 对英文进行分词，也可以用其他的方法，或者默认空格分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['desc_cut'] = data['api_desc'].apply(lambda i:nltk.word_tokenize(text=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>api_id</th>\n",
       "      <th>api_name</th>\n",
       "      <th>api_prim_cate</th>\n",
       "      <th>api_desc</th>\n",
       "      <th>n_followers</th>\n",
       "      <th>n_appear_in_mashup</th>\n",
       "      <th>order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72087</td>\n",
       "      <td>WebPay Direct</td>\n",
       "      <td>4</td>\n",
       "      <td>webteh is croatian based iso/msp/psp registere...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63420</td>\n",
       "      <td>BeenVerified</td>\n",
       "      <td>11</td>\n",
       "      <td>from their site: the beenverified com api foll...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188798</td>\n",
       "      <td>PayPlug</td>\n",
       "      <td>4</td>\n",
       "      <td>the payplug api integrates payments into websi...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70981</td>\n",
       "      <td>Mozilla Persona</td>\n",
       "      <td>11</td>\n",
       "      <td>mozilla persona is an online identity service ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71471</td>\n",
       "      <td>BODC Marsden Square Translator Service</td>\n",
       "      <td>7</td>\n",
       "      <td>the british oceanographic data centre bodc is ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   api_id                                api_name  api_prim_cate  \\\n",
       "0   72087                           WebPay Direct              4   \n",
       "1   63420                            BeenVerified             11   \n",
       "2  188798                                 PayPlug              4   \n",
       "3   70981                         Mozilla Persona             11   \n",
       "4   71471  BODC Marsden Square Translator Service              7   \n",
       "\n",
       "                                            api_desc  n_followers  \\\n",
       "0  webteh is croatian based iso/msp/psp registere...            9   \n",
       "1  from their site: the beenverified com api foll...           17   \n",
       "2  the payplug api integrates payments into websi...            3   \n",
       "3  mozilla persona is an online identity service ...            4   \n",
       "4  the british oceanographic data centre bodc is ...            3   \n",
       "\n",
       "   n_appear_in_mashup  order  \n",
       "0                   0      0  \n",
       "1                   1      1  \n",
       "2                   0      2  \n",
       "3                   0      3  \n",
       "4                   0      4  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将类别转换成数字，如果已经为数字可以不作处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 11,  4, ..., 17, 14,  0], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(data.api_prim_cate.values)\n",
    "data.api_prim_cate.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对数据集进行划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(data.api_desc.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_normalizer(tokens):\n",
    "    \"\"\" 将所有数字标记映射为一个占位符（Placeholder）。\n",
    "    对于许多实际应用场景来说，以数字开头的tokens不是很有用，\n",
    "    但这样tokens的存在也有一定相关性。 通过将所有数字都表示成同一个符号，可以达到降维的目的。\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super(NumberNormalizingVectorizer, self).build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 nltk 里面自带的 英文停用词表\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用刚才创建的NumberNormalizingVectorizer类来提取文本特征，注意里面各类参数的含义\n",
    "tfv = NumberNormalizingVectorizer(min_df=3,  \n",
    "                                  max_df=0.5,\n",
    "                                  max_features=None,                 \n",
    "                                  ngram_range=(1, 2), \n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  stop_words = stopwords.words('english'))\n",
    "\n",
    "# 使用TF-IDF来fit训练集和测试集（半监督学习）\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain)\n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"对数损失度量（Logarithmic Loss  Metric）的多分类版本。\n",
    "    :param actual: 包含actual target classes的数组\n",
    "    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练一个简单的逻辑回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.294 \n"
     ]
    }
   ],
   "source": [
    "#利用提取的TFIDF特征来fit一个简单的Logistic Regression \n",
    "clf = LogisticRegression(C=1.0,solver='lbfgs',multi_class='multinomial')\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7281323877068558"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最后的准确率 72.8 % ，有待进一步优化\n",
    "clf.score(xvalid_tfv,yvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个逻辑回归模型的 准确率是 72.8% 多，对数损失超过了 1 ，可见，性能不是很好，有待进一步优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取文本的 Wordcount 的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(min_df=3,\n",
    "                      max_df=0.5,\n",
    "                      ngram_range=(1,2),\n",
    "                      stop_words = stopwords.words('english'))\n",
    "\n",
    "# 使用Count Vectorizer来fit训练集和测试集（半监督学习）\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.912 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#利用提取的word counts特征来fit一个简单的Logistic Regression \n",
    "\n",
    "clf = LogisticRegression(C=1.0,solver='lbfgs',multi_class='multinomial')\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735224586288416"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(xvalid_ctv,yvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用 Wordcount 提取文本特征，效果比 TF-IDF 要好，有可能是因为文本大多都是短文本，而且每个文本的字数相差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用SVD进行降维，components设为120，对于SVM来说，SVD的components的合适调整区间一般为120~200 \n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "#对从SVD获得的数据进行缩放\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.926 \n"
     ]
    }
   ],
   "source": [
    "# 调用下SVM模型\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "# predict_proba 返回的是一个 n 行 k 列的数组， 第 i 行 第 j 列上的数值是模型预测 第 i 个预测样本为某个标签的概率，并且每一行的概率和为1。\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7198581560283688"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(xvalid_svd_scl,yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
      "662904832/662903077 [==============================] - 454s 1us/step\n",
      "C:\\Users\\superkai\\.keras\\datasets\\multi_cased_L-12_H-768_A-12\\bert_config.json C:\\Users\\superkai\\.keras\\datasets\\multi_cased_L-12_H-768_A-12\\bert_model.ckpt C:\\Users\\superkai\\.keras\\datasets\\multi_cased_L-12_H-768_A-12\\vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import get_pretrained, PretrainedList, get_checkpoint_paths\n",
    "\n",
    "model_path = get_pretrained(PretrainedList.multi_cased_base)\n",
    "paths = get_checkpoint_paths(model_path)\n",
    "print(paths.config, paths.checkpoint, paths.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.07555693, -0.15049051,  0.16471994, ...,  0.77722937,\n",
       "          0.02645287, -0.03034982],\n",
       "        [-0.1284019 , -0.35628211,  0.02628936, ...,  1.4060163 ,\n",
       "          0.16582859, -0.29848817],\n",
       "        [-0.26589885, -0.25956875,  0.11313999, ...,  1.3601148 ,\n",
       "          0.10190892, -0.4161586 ],\n",
       "        ...,\n",
       "        [-0.34311667, -0.25090867, -0.68278617, ...,  1.3271173 ,\n",
       "         -0.15756905, -0.47915685],\n",
       "        [-0.24629438, -0.1237767 , -0.06864089, ...,  1.3978268 ,\n",
       "         -0.03948718, -0.20072535],\n",
       "        [-0.02701636, -0.24944861,  0.3372642 , ...,  0.7892858 ,\n",
       "          0.05166285, -0.12041575]], dtype=float32),\n",
       " array([[ 0.30147606, -0.22503024,  0.6198242 , ...,  0.565196  ,\n",
       "         -0.0383291 ,  0.1545777 ],\n",
       "        [ 0.10668586, -0.26670808,  0.65882355, ...,  1.0995189 ,\n",
       "         -0.13540927,  0.09595349],\n",
       "        [-0.29448897, -0.3521183 ,  0.45152533, ..., -0.2449576 ,\n",
       "         -0.4460358 , -0.0800565 ],\n",
       "        ...,\n",
       "        [ 0.42974818, -0.32283914,  0.4397599 , ...,  0.17552848,\n",
       "         -0.41719222, -0.0893782 ],\n",
       "        [ 0.47157127, -0.30103415,  0.68742204, ...,  0.34941345,\n",
       "         -0.12127692, -0.11517797],\n",
       "        [ 0.2623538 , -0.16330388,  0.83729744, ...,  0.5393054 ,\n",
       "         -0.05221995,  0.0438977 ]], dtype=float32)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_bert import extract_embeddings\n",
    "\n",
    "model_path = 'C:/Users/superkai/.keras/datasets/multi_cased_L-12_H-768_A-12'\n",
    "texts = ['all work and no play', 'makes jack a dull boy~']\n",
    "\n",
    "embeddings = extract_embeddings(model_path, texts)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 768)\n",
      "(10, 768)\n"
     ]
    }
   ],
   "source": [
    "for i in embeddings:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "bc.encode([\"I like python!\",\"I hate write paper!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
